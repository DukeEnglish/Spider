{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebCrawl-Tutorial\n",
    "\n",
    "-By Junyi Li\n",
    "\n",
    "写在读者：永远对别人的话／文字保持警惕，请带着自己的思考去看待一切，才能更好的成长与学习。\n",
    "\n",
    "世道必进，后胜于今 --- 严复\n",
    "\n",
    "本次的教程，依据别人的课程教学进行。禁止用于商业用途，仅供学习交流\n",
    "这个是针对有一定的编程基础的同学。这个教程中我引用了很多网上的blog的内容，所以有些地方就会直接链接到别人的主页以保护著作人的权益。\n",
    "由于作者才疏学浅，至今对于爬虫的接触满打满算也就两周多两天，所以在tutorial的写作中不免有很多很多的问题，如果你没有看明白，不要犹豫，绝对是我的锅！！！希望看到的同学们有疑问请当面指出，不胜感激！\n",
    "\n",
    "Contact:ljyduke@gmail.com\n",
    "\n",
    "https://github.com/DukeEnglish/Web-crawler-engineer-for-Python\n",
    "\n",
    "## 目录\n",
    "\n",
    "- Chapter 1 爬虫概述及印象描述\n",
    "- Chapter 2 数据解析及网页分析\n",
    "- Chapter 3 三个例子以及针对不同的情况需要的一些技巧\n",
    "- Chapter 4 Scrapy 介绍以及一个例子\n",
    "- Chapter 5 待扩展\n",
    "\n",
    "\n",
    "## 概述\n",
    "\n",
    "因为我们在从事AI相关的任务的时候，我们会需要很多的数据来进行模型训练。所以数据的多少，质量的高低会在很大程度上影响我们最后所能够得到的效果。所以在无法得到公开数据集的时候，爬虫就显得非常必要了。\n",
    "\n",
    "在爬虫之后，我们可以获得想要的数据。其次我们可以进一步做数据分析以及数据可视化。经过对数据的初步处理之后就可以进行机器学习训练。\n",
    "\n",
    "### 爬虫流程\n",
    "\n",
    "- 选取挑选的种子URL\n",
    "- 将这些URL放入等待抓取的URL队列\n",
    "- 取出待抓取的URL，下载，存储进已经下载的网页库中。此外，将这些URL放入已经抓取的URL队列\n",
    "- 分析已经抓取队列中的URL，并将URL放入待抓取的URL队列，从而进入下一个循环\n",
    "\n",
    "### 如何编写爬虫\n",
    "\n",
    "- 语言：Python\n",
    "- 流程：\n",
    "\t- 获得源码：Urllib、Requests\n",
    "\t- 解析源码：Beautiful Soup、正则表达式\n",
    "\t- 保存数据：Xls，MySQL，MongoDB \n",
    "    \n",
    "爬虫的知识体系其实和全栈工程师的知识体系非常相似。\n",
    "\n",
    "- 前端html，css，js，浏览器相关知识；\n",
    "- 各种数据库的运用\n",
    "- http协议的了解\n",
    "- 对于前后台联动的方案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是两个简单的爬取百度的爬虫示例\n",
    "使用urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "# import the lib: urllib.request\n",
    "f= urllib.request.urlopen('http://www.baidu.com/')\n",
    "# open this website and return a object of class\n",
    "f.read(500)\n",
    "# print the first 500 character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.read(500).decode('utf-8')\n",
    "# 将前500行内容以utf-8的编码格式解码并打印"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用request库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "r = requests.get('http://www.baidu.com/')\n",
    "r.text# 获取其中的文本对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r.encoding = 'utf-8'\n",
    "r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r.content # 这样的命令就是获取二进制的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来看一下爬虫的操作步骤：\n",
    "\n",
    "- 获取数据\n",
    "- 解析数据\n",
    "- 保存数据\n",
    "\n",
    "我们刚才所做的事情就是获取数据，并没有对他做任何的解析。接下来我们针对豆瓣做一个介绍。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # 获取数据\n",
    "import requests\n",
    "import lxml\n",
    "\n",
    "r = requests.get('http://book.douban.com/subject/1084336/comment/').text\n",
    "\n",
    "#解析数据\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(r)# lxml是解析方式\n",
    "pattern=soup.find_all('p','comment-content')# 找到所有标签为p，类属性为comment-content的内容\n",
    "\n",
    "for item in pattern:\n",
    "    print(item.string)\n",
    "\n",
    "# 保存数据 保存数据主要是看需求，可以直接输出，可以输出各种各样的格式，可以保存到数据库中。\n",
    "\n",
    "import pandas\n",
    "comments = []\n",
    "\n",
    "for item in pattern:\n",
    "    commnets.append(item.string)\n",
    "    \n",
    "df = pandas.DataFrame(comments) #将数据保存到DataFrame中\n",
    "df.to_csv('comments.csv')#导出到这里"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url ='http://book.douban.com/subject/1084336/comments'\n",
    "r = requests.get(url,timeout = 20)\n",
    "# print(r.text)\n",
    "# print(r.raise_for_status())\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们对Requests对象的一些属性进行简单的介绍：\n",
    "\n",
    "r.status_code # http请求的返回状态，200表示连接成功，404表明访问url无效，403表明没有权限（一般来说是爬虫爬得不好被封了）\n",
    "\n",
    "r.text # 返回对象的文本内容\n",
    "\n",
    "r.content # 返回对象的二进制形式\n",
    "\n",
    "r.encoding # 返回对象的编码方式\n",
    "\n",
    "r.apparent_encoding # 相应内容编码方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是一个通用函数，可以被用来爬取得到整个网页"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def getHtmlText(url):\n",
    "    try:\n",
    "        r = requests.get(url,timeout = 20)\n",
    "        r.raise_for_status()\n",
    "        r.encoding = r.apparent_encoding\n",
    "        return r.text\n",
    "    except:\n",
    "        return \"产生异常\"\n",
    "    \n",
    "url ='http://book.douban.com/subject/1084336/comments'\n",
    "getHtmlText(url) # 将爬到的网页展示出来，这个和我们直接看到的网页源码是一样的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "robots协议。是一个爬虫应该遵守的协议。一般来说，我们个人爬去数据做一定的实验学习影响并不大，一般来说像是百度等大型的网站是一定要遵守这个协议的。曾经百度因为360不遵守协议所以有一场官司。（PS：建议去看看知乎的协议，很有趣^ _ ^）\n",
    "\n",
    "https://baike.baidu.com/item/robots%E5%8D%8F%E8%AE%AE/2483797?fromtitle=robots.txt&fromid=9518761\n",
    "\n",
    "如何查看呢？ 只要在主域名的最后加一个robots.txt 就可以找到。强烈建议先看看这个在进行爬取哦～～～会有惊喜\n",
    "\n",
    "当然不排除有一些网站并不提供这个协议，那就意味着你可以随意抓取，前提是你不被人家网站给反掉\n",
    "\n",
    "所以我们正确的爬虫更详细的应该是，\n",
    "- 找到目标网址\n",
    "- 查看robots.txt\n",
    "- 获取数据\n",
    "- 解析数据\n",
    "- 保存数据\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到这里，我想你对于爬虫应该有了自己的一个模糊的认知。如果对http有需要了解的话，此时请转到这个教程中：https://foofish.net/understand-http.html 这里有一定的简单介绍。此时请看Chapter2\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
