{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 解析数据\n",
    "\n",
    "爬虫的第一步是获取到网页的数据，我们之前使用的是requests。然后如何对内容进行解析呢？可以使用Beautifulsoup，正则表达式以及lxml。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我在这里简单的说一下xpath的并且给了一个例子。可是这个我觉得真的挺简单，随便走走应该会懂的。我之后在研究一下看看怎么说会好一些。Xpath 的语法非常的简单，可以自己上网去学习。如果想要获取文本内容，使用text()。 如果想要获取到注释，使用comment()。 如果要获取其他的别的属性使用@xx（xx为属性）： \n",
    "\n",
    "- @href\n",
    "- @src\n",
    "- @value\n",
    "\n",
    "如果想要获取某个标签下所有的文本，包括子标签下的文本，使用string：\n",
    "\n",
    "例如： <p>123<a> 内容 </a></p> 这里如果想要得到文本 “123内容”， 则需要使用string。\n",
    "\n",
    "另外 strats-with 匹配字符串前缀相等\n",
    "\n",
    "contains 匹配任何位置相等\n",
    "\n",
    "\n",
    "有一些地方会告诉你从浏览器中直接复制，这个是可以的，缺点是如果路径很长的话会导致你的代码很长。这个时候如果自己来解析就会快很多。这里掌握两个技巧\"//\"表示从页面任何位置开始 \"./\" 表示从当前位置开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "\n",
    "url = \"https://book.douban.com/subject/1084336/comments/\"\n",
    "r = requests.get(url).text\n",
    "\n",
    "s = etree.HTML(r)\n",
    "\n",
    "print(s.xpath('//div[@class=\"comment\"]/p/text()'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于保存数据的方法这里就不讲了，这个是python的基础知识。下面给出了一些示例代码进行文件操作\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('name_of_file','r/w','utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        print(line)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame(np.random.randn(6,3))\n",
    "df.to_csv('name_of_your_csv')\n",
    "df.to_excel(\"name_of_you_excel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "\n",
    "url = 'https://book.douban.com/subject/1084336/comments/'\n",
    "r = requests.get(url).text\n",
    "\n",
    "s= etree.HTML(r)\n",
    "file = s.xpath('//div[@class=\"comment\"]/p/text()')\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(file)\n",
    "df.to_excel(\"name_of_you_excel2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "哈哈，到这里就差不多具备了一些零散的知识啦。然后要上手去爬！怎么爬呢？爬虫参考博客：强力推荐，去看视频，不要浪费大量时间去摸索这个……没有意义，亲测。\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/25102367\n",
    "https://zhuanlan.zhihu.com/p/25017443\n",
    "https://zhuanlan.zhihu.com/p/25546414\n",
    "\n",
    "感谢张宏伦博士的无私分享。在页面最下方有视频链接，这也是我强推大家去看的目的！！！如果大家哪里不明白就问我，I'm here!\n",
    "\n",
    "然后！去看Chapter3 吧，任务就是看每一行代码，然后大概明白他们在做什么就OK啦！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
