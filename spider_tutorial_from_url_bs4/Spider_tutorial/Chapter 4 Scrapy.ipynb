{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy\n",
    "\n",
    "我个人不建议直接去阅读其官方文档，我建议可以直接上手做。然后在中间不懂的时候再去查阅文档。当然有人也喜欢这样子。所谓因材自学。\n",
    "\n",
    "下面是三张图片，表示scrapy这个框架的整个结构。其本质和我们之前讲到的东西是一致的，区别就在于这个框架将很多东西都封装好了，你只需要实现某些特定的功能就ok。我想几乎所有的人都知道tensorflow吧。嗯就是这样子的，你手写一个神经网络，fp，bp，对应的就是我们之前写的那些requests还有beautifulsoup等。tensorflow就对应scrapy。照葫芦画瓢很简单，但是原理还是应该先了解。\n",
    "\n",
    "![title](components.jpeg)\n",
    "![title](Nature.jpeg)\n",
    "![title](Nature2.jpeg)\n",
    "\n",
    "前面的部分理论比较重，最后会有一个推荐资料去做一些实例。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我会对这个框架的构架做一个说明和解释。里面有7个部件是比较重要的：\n",
    "\n",
    "- Scrapy Engine 引擎：\n",
    "- Scheduler 调度器\n",
    "- Downloader 下载器\n",
    "- Spider 蜘蛛： 爬虫，做数据匹配，来抓的代码\n",
    "- Item Pipelines 项目管道： 做数据清洗\n",
    "- Downloader Middlewares 下载器中间件\n",
    "- Spider Middlewares 蜘蛛中间件\n",
    "\n",
    "其中Spider 和 Item pipeline是最基础的。最重要的是下载器中间件以及蜘蛛中间件。其中下载器中间件是我们需要了解比较多的，我们通过它连接到了互联网。\n",
    "\n",
    "调度器，引擎，下载器暂时还不需要了解很多，因为我们很少会主动会对他们进行操作。其中调度器还是有一些可能的。\n",
    "\n",
    "所以大量的反爬的操作都是在下载器中间件中完成的。\n",
    "- 设计反爬\n",
    "- 设计防范机制（防止爬取到无效数据）\n",
    "\n",
    "最后管道就是用来做储存的，或者说做储存之类的操作。\n",
    "\n",
    "下载器中间件是我们需要做最多操作的地方，对于反爬机制等很重要的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scrapy 数据流通\n",
    "\n",
    "Scrapy中的数据流由执行引擎控制，其过程如下:\n",
    "\n",
    "1、引擎打开一个网站(open a domain)，找到处理该网站的Spider并向该spider请求第一个要爬取的URL(s)。\n",
    "\n",
    "2、引擎从Spider中获取到第一个要爬取的URL并在调度器(Scheduler)以Request调度。\n",
    "\n",
    "3、引擎向调度器请求下一个要爬取的URL。\n",
    " \n",
    "4、调度器返回下一个要爬取的URL给引擎，引擎将URL通过下载中间件(请求(request)方向)转发给下载器(Downloader)。\n",
    " \n",
    "5、一旦页面下载完毕，下载器生成一个该页面的Response，并将其通过下载中间件(返回(response)方向)发送给引擎。 \n",
    "\n",
    "6、引擎从下载器中接收到Response并通过Spider中间件(输入方向)发送给Spider处理。\n",
    " \n",
    "7、Spider处理Response并返回爬取到的Item及(跟进的)新的Request给引擎。\n",
    " \n",
    "8、引擎将(Spider返回的)爬取到的Item给Item Pipeline，将(Spider返回的)Request给调度器。 \n",
    "\n",
    "9、(从第二步)重复直到调度器中没有更多地request，引擎关闭该网站。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy startproject name_of_project 创建项目，在命令行下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd name_of_project\n",
    "scrapy genspider example example.com\n",
    "# 其中example可以更换为你想要的名字，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "好的，我们来讲解一下生成的爬虫的各个属性，方法的作用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class Spiderexample(scrapy.Spider):\n",
    "    name = 'name_of_project' # 项目的名称，由于一个project下可以有很多爬虫类，而这里的name就是为了区分不同的爬虫。所以不能重复的\n",
    "    allowed_domain = ['58.com'] # 允许爬取的主域名\n",
    "    start_urls = ['http://58.com/'] # 开始爬取的url \n",
    "    # start_request 这个方法如果不重写，那么就会按照下面我们写的流程运行。如果重写啦就可能不是这样啦～～～\n",
    "    def parse(self. response):\n",
    "        pass\n",
    "'''\n",
    "所以当我们启动一个爬虫 spider crawl name_of_project的时候\n",
    "首先从start_urls中读取url\n",
    "然后自动调用start_request函数\n",
    "最后这个函数请求的结果自动调用默认解析器parse。\n",
    "'''\n",
    "\n",
    "'''\n",
    "如果我们重写了start_request，那么就会有变化\n",
    "当然我们也可以自己调用别的解析器。这个在之后的项目中会见到\n",
    "这也是为什么我们一创建这个爬虫就有了这些东西\n",
    "'''    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy import cmdline\n",
    "\n",
    "cmdline.execute('scrapy crawl name_of_project'.split(' '))\n",
    "\n",
    "或者 \n",
    "\n",
    "cmdline.execute(['scrapy'],['crawl'],['name_of_project'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来是对于选择器的讲解\n",
    "我们有四种选择器：css,xpath,re,pyquery。\n",
    "\n",
    "一般来说css和pyquery(Qquery)东前端的都懂据说，但是我不懂hhhhh。所以我倾向于xpath和re\n",
    "\n",
    "css,xpath,re是scrapy内置的三种，所以这个其实挺方便的\n",
    "\n",
    "css\n",
    "应用给定的css选择器，返回SelectorList的一个实例，query是一个包含CSS选择器的字符串，在后台，通过cssselect库和运行.xpath()方法，CSS查询会被转换为XPath\n",
    "\n",
    "ps：方便起见，这个方法也可以通过response.css()调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用selector\n",
    "\n",
    "from scrapy import Selector\n",
    "\n",
    "# id在html页面中有唯一性，所以如果我们使用id来查找特定的内容，那么就可以准确定位到\n",
    "\n",
    "sel = Selector(test)\n",
    "\n",
    "a = sel.xpath('/html/body') # 然后继续往后写就好了，注意／是下一级，／／是任意起点\n",
    "# xpath 和文件路径的思想是一样的，所以按照文件路径的思路来写路径就ok了\n",
    "# 除了在开头的时候，二次继续选择的时候需要标注当前路径（使用 .／ = Linux 下的操作方式）\n",
    "\n",
    "# 那么如何提取其中的内容呢？ 使用extract() 方法\n",
    "\n",
    "a.extract()# 就取到了a的文本内容\n",
    "a.extract_first() # 取到了a的第一个文本\n",
    "\n",
    "'''\n",
    "另外我们可以使用@来获取它的类属性\n",
    "'''\n",
    "\n",
    "a.xpath('./@class').extract_first() #就可以将class的内容取出来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item Pipeline\n",
    "\n",
    "- 主要负责处理蜘蛛从网页中抽取到的item，做清洗，验证和存储数据\n",
    "- 每个item pipeline的组件都是由一个简单的方法组成Python类\n",
    "- 这些类获取到了item，并执行自身的方法，同时还需要确定是否需要在item pipeline中继续执行下一步或者不做处理\n",
    "- 对数据进行校验，去重等操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Item pipeline的主要函数:\n",
    "\n",
    "- process_item(self, item, spider) --- 必须实现\n",
    "    \n",
    "- open_spider(self, spider) --- 非必需，为爬虫启动的时候调用\n",
    "- close_spider(self, spider) --- 非必需，爬虫关闭的时候调用\n",
    "- from_crawler(cls, crawler) --- 非必需，在open_spider之前调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.exceptions import DropItem\n",
    "# 这段代码非常简单，来自官网，我们这里只需要关注最后一行\n",
    "class PricePipeline(object):\n",
    "    vat_factor = 1.15\n",
    "    \n",
    "    def process_item(self, item, spider):\n",
    "        if item['price']:\n",
    "            if item['price_excludes_vat']:\n",
    "                item['price'] = item['price'] * self.vat_factor\n",
    "            return item\n",
    "        else:\n",
    "            raise DropItem('Missing price in %s' % item) # 这一行做了什么呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DownloadMiddleware\n",
    "\n",
    "一般我们不在这里做数据校验，因为我们收集到的response千奇百怪。所以我们在这里要判断返回的数据是不是被反爬的一些东西（例如，返回的状态码不对，这种情况下可以对这个相应的url做一些处理【403被封了，就等半个小时一班就开了】）\n",
    "\n",
    "下载器中间件在爬虫中会起到非常重要的作用\n",
    "\n",
    "下载器中间件是我们做反爬的最重要的一个组件\n",
    "\n",
    "PS： 下面的呃返回值是什么，就会让对应的方法来做处理。如果是返回none，就继续走，直到所有的中间件走完。（每个中间件有这三个方法）\n",
    "\n",
    "- process_request(request, spider) 是主要函数\n",
    "    返回值：\n",
    "    \n",
    "        - return None ： 就是没有什么问题，将request送出去到下载器，可以连接到互联网\n",
    "        - return response： 返回一个response， 让process_response 来处理这个\n",
    "        - return request ： 就是request有问题，不能被送出去，被送回了scheduler （基于某种判断）\n",
    "        - raise IgnoreRequest ： 将某些链接抛弃（基于某种判断过滤机制）\n",
    "- process_response(requet, response, spider)\n",
    "    对response做处理\n",
    "        \n",
    "        - return response: 将返回的信息返回\n",
    "        - return request： 将request返回回去，重新做一次处理 （同上面的return resquest）\n",
    "        - raise IgnoreRequest： 将返回的response忽视掉，抛弃\n",
    "- process_exception(request, exception, spider)\n",
    "    对异常情况的处理 （包括ignorerequest）\n",
    "    \n",
    "        - return None 继续执行，让别的process_exception 来处理这个异常\n",
    "        - return response 就会让别的 process_response 来处理这个response\n",
    "        - return request 同上\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "另外关于scrapy自己的url去重系统。请求方法名+url 做成一个子文，存入队列。跑过之后就不跑了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapy 项目上手实践\n",
    "\n",
    "推荐资料：https://zhuanlan.zhihu.com/p/24669128\n",
    "\n",
    "这里要注意，对于小型的项目，你怎么写都没有关系。但是我建议从一开始就养成分模块编程，让每一个模块做它应该做的事情的习惯，这样才是正确的使用了这个框架。不然……你干什么要用他。\n",
    "\n",
    "关于这个项目的别的实例。\n",
    "\n",,
    "\n",
    "另一个是目录下的sec_mafengwo\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
